{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4cab8d8-fc5b-4a4c-b6b2-907d19766f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Running a Streaming Pipeline from S3 to a Delta Table\n",
    "\n",
    "In this notebook we are going to go through a step-by-step pipeline to stream files from an s3 bucket/folder to a Delta Table, using an append-only mode.  The [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) provides an introduction to all the bells and knobs of stream.  \n",
    "\n",
    "The pipeline pattern we want to achieve to the reading, transformation, and writing of json objects in an S3 Bucket.  We want this pipeline to be idempotent with respect to reading (an s3 object is only read and processed once) and writing (the rows in the object are only appended to the Delta table once).  Structured streaming gives us a bunch of these guarantees (and more).\n",
    "\n",
    "To make the process a little \"easier\" the Metis-Job (yes, another Ancient Greek Diety) provides a set of abstractions to encapsulate all the Spark and Databricks options.  Using Metis-Job, we can also test this process locally, with only a pyspark environment at our side.  This is not perfect; the stream reader on databricks (based on the [Autoloader](https://docs.databricks.com/en/ingestion/auto-loader/index.html) functionality; called cloud files) can't be run outside Databricks.  However, the [recursive file lookup](https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#recursive-file-lookup) native to Spark, can.\n",
    "\n",
    "We are going to be working in the `hacking_data` Unity catalogue, in a schema (or database, if you will) called `delta_test`.  The table we'll be writing to is `sketches`; data on various Monty Python sketches (big fans of Dead Parrots, registering fish, and Spark Streaming).  In the world of Unity catalogues, this makes the path to the table `hacking_data.delta_test.sketches`.\n",
    "\n",
    "And we'll start by importing from pyspark functions, delta, and metis_job.\n",
    "\n",
    "## Notes on Running the Demo\n",
    "\n",
    "* There is event data loaded into a managed catalogue volume; here `/Volumes/hacking_data/delta_test/streaming_events`.  This contains 2 json-like files (actually JSON-lines), each file contains 2 rows.\n",
    "* The destination delta table is here `hacking_data.delta_test.sketch`.  Before the start of the demo, just delete it.\n",
    "* The checkpoints sit in another managed volume `/Volumes/hacking_data/delta_test/checkpoints/sketch_events/`.  Before starting the demo, the `sketch_events` folder needs to be removed.  To do this, `dbutils.fs.rm(job_cfg.checkpoint_path, True)`\n",
    "* Also, trying running the stream a few times (without deleting the table and checkpoints between each run) see how the idempotent aspects work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa6c267-426f-47d5-8c4f-66a8f0da662c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "import metis_job\n",
    "from delta.tables import *\n",
    "\n",
    "# sess = metis_job.build_spark_session('a', session.create_connect_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59805f1c-b91a-44c7-9572-ee873252ee01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## The Constructors\n",
    "\n",
    "If, if this was a job, alot of this metis_job plating-of-the-boiler would be hidden away in the model layer.  Here, we'll be explicit about defining the following:\n",
    "* A term vocab to use for the schema; sort of like a simple term translater.\n",
    "* We are going to define 2 schemas.  The schema definition used in the pipeline is a [Spark StructType instance](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html).  However, instead of coding the schema drectly in Spark StructTypes, metis_job provides an abstraction; `metis_job.Schema`.  Schema is a fluent API for constructing the various Spark Struct types, like `StructField`, `StructString`, as well as nested `StructType`.\n",
    "  * The event source schema.  The schema of the json docs in the s3 bucket; the function `sketches_event_schema`.\n",
    "  * The destination table schema.  The schema of the delta table; the function `sketches_schema`.  This is slightly different from the event schema, as during the streaming we transform the source events.\n",
    "* The `Sketch` class.  It is a type of `DomainTable`, provided by metis_job, which abstracts away a number of base Spark and Delta functions, such as creating the table, reading, performing Delta merges and upserts, and, in this example, providing table info to the streaming functions.  There is a bunch of configuration in the `Sketch` class.  Essentially, `DomainTable` provides common Spark table functions, such as setting properties.  More importantly, it defines DeltaTable functions, such as partitioning, pruning, and functions like the infamous Delta upsert (that `identity_merge_condition` function provides the upsert rule for the delta table).  We're not using Delta upserts in this example, ony a delta append.\n",
    "* The `build_job_config` function is designed for a job.  It provides the base configuration for a Spark job, and is primarily used to describe the namespace (the Unity Catalogue and its component parts) for the job.  More about that below.  Notice that the catalogue is `hacking_data` as we mentioned above.  The schema is `delta_test`, except its called `data_product`, beacuse the Unity Catalogue schema is our Data Mesh data product concept.\n",
    "* The namespace (the schema in the catalogue) is defined, this takes the job config and the current Spark session.\n",
    "* Finally, we initialise the `Sketch` table object.  The `build_namespace()` call, creates the Cataloge schema if it doesn't exist.  Finally, we're asked `Sketch` to create (if not exists) a managed Delta Table (`table_creation_protocol = metis_job.CreateManagedDeltaTable`), which gets called as an after object initialisation callback (`after_initialise` function).\n",
    "\n",
    "We're done with constructing our destination catalogue, schema and table.  Now for the streaming configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbb7d97-f346-4f08-8711-25c363fccf0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def vocab():\n",
    "    return {\n",
    "        \"sketches\": {\n",
    "            \"isDeleted\": {\n",
    "                \"term\": \"isDeleted\"\n",
    "            },\n",
    "            \"name\": {\n",
    "                \"term\": \"name\"\n",
    "            },\n",
    "            \"pythons\": {\n",
    "                \"term\": \"pythons\"\n",
    "            },\n",
    "            \"season\": {\n",
    "                \"term\": \"season\"\n",
    "            },\n",
    "            \"source_file\": {\n",
    "                \"term\": \"source_file\"\n",
    "            },\n",
    "            \"processing_time\": {\n",
    "                \"term\": \"processing_time\"\n",
    "            }\n",
    "        },\n",
    "        \"columns\": {\n",
    "            \"column1\": {\n",
    "                \"term\": \"column_one\"\n",
    "            },\n",
    "            \"column2\": {\n",
    "                \"term\": \"column_two\",\n",
    "                \"sub1\": {\n",
    "                    \"term\": \"sub_one\"\n",
    "                },\n",
    "                \"sub2\": {\n",
    "                    \"term\": \"sub_two\"\n",
    "                },\n",
    "                \"sub3\": {\n",
    "                    \"term\": \"sub_three\",\n",
    "                    \"sub3-1\": {\n",
    "                        \"term\": \"sub_three_one\"\n",
    "                    },\n",
    "                    \"sub3-2\": {\n",
    "                        \"term\": \"sub_three_two\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"*\": {\n",
    "            \"@id\": {\n",
    "                \"term\": \"id\"\n",
    "            },\n",
    "            \"@type\": {\n",
    "                \"term\": \"type\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def sketches_event_schema():\n",
    "    sketch_event_table = (metis_job.Schema(vocab=vocab())\n",
    "                          .column()  # id\n",
    "                          .string(\"*.@id\", nullable=True)\n",
    "\n",
    "                          .column()  # is_deleted\n",
    "                          .string(\"sketches.isDeleted\", nullable=True)\n",
    "\n",
    "                          .column()  # name\n",
    "                          .string(\"sketches.name\", nullable=True)\n",
    "\n",
    "                          .column()  # pythons array\n",
    "                          .array_struct(\"sketches.pythons\", nullable=True)\n",
    "                          .string(\"*.@id\", nullable=True)\n",
    "                          .end_struct()\n",
    "\n",
    "                          .column()  # season\n",
    "                          .string(\"sketches.season\", nullable=True))\n",
    "    return sketch_event_table\n",
    "\n",
    "\n",
    "def sketches_schema():\n",
    "    sketch_table = (metis_job.Table(vocab=vocab())\n",
    "                    .column()  # id\n",
    "                    .string(\"*.@id\", nullable=True)\n",
    "\n",
    "                    .column()  # is_deleted\n",
    "                    .string(\"sketches.isDeleted\", nullable=True)\n",
    "\n",
    "                    .column()  # name\n",
    "                    .string(\"sketches.name\", nullable=True)\n",
    "\n",
    "                    .column()  # pythons array\n",
    "                    .array_struct(\"sketches.pythons\", nullable=True)\n",
    "                    .string(\"*.@id\", nullable=True)\n",
    "                    .end_struct()\n",
    "\n",
    "                    .column()  # name\n",
    "                    .string(\"sketches.season\", nullable=True)\n",
    "\n",
    "                    .column()  # source_file\n",
    "                    .string(\"sketches.source_file\", nullable=True)\n",
    "\n",
    "                    .column()  # processing_time\n",
    "                    .timestamp(\"sketches.processing_time\", nullable=True))\n",
    "    return sketch_table\n",
    "\n",
    "\n",
    "class Sketch(metis_job.DomainTable):\n",
    "    table_name = \"sketch\"\n",
    "\n",
    "    table_creation_protocol = metis_job.CreateManagedDeltaTable\n",
    "\n",
    "    schema = sketches_schema().hive_schema()\n",
    "\n",
    "    partition_columns = (\"name\",)\n",
    "\n",
    "    pruning_column = 'name'\n",
    "\n",
    "    table_properties = [\n",
    "        metis_job.TableProperty(metis_job.DataAgreementType.SCHEMA_VERSION, \"0.0.1\", \"my_namespace\"),\n",
    "        metis_job.TableProperty(metis_job.DataAgreementType.PARTITION_COLUMNS, \"identity\", \"my_namespace\"),\n",
    "        metis_job.TableProperty(metis_job.DataAgreementType.PRUNE_COLUMN, \"identity\", \"my_namespace\"),\n",
    "        metis_job.TableProperty(metis_job.DataAgreementType.PORT, \"superTable\", \"my_namespace\"),\n",
    "        metis_job.TableProperty(metis_job.DataAgreementType.UPDATE_FREQUENCY, \"daily\", \"my_namespace\"),\n",
    "        metis_job.TableProperty(metis_job.DataAgreementType.DESCRIPTION, \"Some description\", \"my_namespace\"),\n",
    "    ]\n",
    "\n",
    "    def after_initialise(self):\n",
    "        self.perform_table_creation_protocol()\n",
    "\n",
    "    def identity_merge_condition(self, name_of_baseline, update_name):\n",
    "        return f\"{name_of_baseline}.id = {update_name}.id\"\n",
    "\n",
    "def build_job_config():\n",
    "    checkpoint_volume = \"/Volumes/hacking_data/delta_test/checkpoints/sketch_events\"\n",
    "    return metis_job.JobConfig(\n",
    "        service_name=\"mock_job\",\n",
    "        catalogue=\"hacking_data\",\n",
    "        data_product=\"delta_test\",\n",
    "        job_mode=metis_job.JobMode.UNITY,\n",
    "        checkpoint_location=checkpoint_volume)\n",
    "\n",
    "def build_namespace(cfg):  \n",
    "    return metis_job.NameSpace(session=spark,\n",
    "                               job_config=cfg)\n",
    "\n",
    "\n",
    "job_cfg = build_config()\n",
    "\n",
    "sketches = Sketch(namespace=build_namespace(job_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19da32f-fec0-4698-a3b2-42098eee2b58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e68f23d6-8574-4648-b8b9-07e29222833d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Constructing the Streaming Pipeline\n",
    "\n",
    "We're using the Databricks Autoloader functionality to stream files from S3.  This is configured in Databricks Spark as \"cloudFiles\".  Metis_job has a simple abstraction for this called `CloudFiles` (well, why not?). This works both in a local PySpark environment (by injecting different dependencies and configuration) and in Databricks.\n",
    "\n",
    "The main configuration aspects of the stream are:\n",
    "* `stream_reader`.  An instance of `stream_reader=metis_job.DatabricksCloudFilesStreamer`.  We initialise it with a Spark option defining the type of files to be streamed; in this case JSON.\n",
    "* `cloud_source`.  This is the location of the source events.  In the case of this example, this is a managed volume within the `delta_test` schema (which is still s3 under the rather vast covers), but it could easily be an external volume configured as a Volume in the catalogue.  For an example of one of those, see `/Volumes/domain/telemetry/telemetry-events`\n",
    "* `checkpoint_location`.  For our S3 volume the checkpoint location is where the stream writes progress data for fault-tolerance and idempotency purposes.\n",
    "* `schema` is the previously defined event schema.  Streaming processign can infer the schema by reading ahead.  However, more complex schema (our 1 has an array struct) this doesn't work well.  Making the schema explicit reduces cdata compatibility issues as well.\n",
    "* `stream_writer` is an instance of the `metis_job.DeltaStreamingTableWriter` class.  It is currently opinionated about 1 thing; the [trigger condition](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers) is `{'availableNow': True}`\n",
    "* Finally, we tell it to stream to the `Sketch` Delta table we created above.  In Delta Stream, all the stream writer needs is the fully qualified table name; `hacking_data.delta_test.sketch` in our case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b712c353-98c3-4ab8-9bb8-7ae21805d067",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_reader_opts = [metis_job.SparkOption.JSON_CLOUD_FILES_FORMAT]\n",
    "stream_from_location = \"/Volumes/hacking_data/delta_test/streaming_events\"\n",
    "\n",
    "# dbutils.fs.rm(job_cfg.checkpoint_location, True)`\n",
    "\n",
    "cloud_files = metis_job.CloudFiles(spark_session=spark,\n",
    "                                   stream_reader=metis_job.DatabricksCloudFilesStreamer(stream_reader_opts),\n",
    "                                   cloud_source=stream_from_location,\n",
    "                                   checkpoint_location=job_cfg.checkpoint_location,\n",
    "                                   schema=sketches_event_schema().hive_schema(),\n",
    "                                   stream_writer=metis_job.DeltaStreamingTableWriter(),\n",
    "                                   stream_to_table=sketches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78df63e9-7d48-4673-9f1b-f6bb29b69fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setting up the Stream\n",
    "\n",
    "The `read_stream` function sets up the stream by invoking the `read_stream` on `metis_job.DatabricksCloudFilesStreamer`.  This goves us back a Dataframe.  But a DF which is in streaming mode.  This means that we ony use a subset of Spark functions on the DF; for instance, we can't use any of the action functions (like `collect` or `take`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d160b411-bbe3-49b4-bffb-5373f7e9f7a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = cloud_files.read_stream()\n",
    "\n",
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8b488b-1bfa-4fa3-a539-64cca2a75bef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations\n",
    "\n",
    "What we can do, however, is to perform transformation functions.  There are some limits when streaming, but most [transformations are supported](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#operations-on-streaming-dataframesdatasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5438a61f-c1ea-4d68-8920-e4c445bcdf9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumns({'source_file': col(\"_metadata.file_path\"),\n",
    "                          'processing_time': current_timestamp()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1cb3370-1b9b-49fc-8c19-a105906bbdaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write The Stream\n",
    "\n",
    "We are using the `metis_job.DeltaStreamingTableWriter` stream writer, which appends the transformed rows generated by the stream to the delta table defined in the `cloud_files` configuration. Writing to the stream essentially starts the stream processing; the previous 2 steps didn't activate the stream.  \n",
    "\n",
    "The started stream returns a `StreamingQuery` object to allow tracking of the job.  In `DeltaStreamingTableWriter`, it calls `awaitTermination()` on the resulting StreamingQuery object. Then returns the object.  We can then check the stream's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7bd781-b8ec-4b51-a6ce-5ff446f4f97c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_query = cloud_files.write_stream(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb78866-3529-4739-aa38-74113936092d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {}
    }
   ],
   "source": [
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2873a3a-cecf-4dbd-b681-3eef15e88761",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Checking our Stream\n",
    "\n",
    "Finally, we can check the data written from the events to the Delta table with a common Spark read operation.  Our `Sketch` object takes care of the details via the `read` function, which returns a DF of the Delta Table.  Finally we show what's there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b59568d-f006-4b45-810e-f1daaa8dc3ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+---------+--------------------+--------------------+------+--------------------+--------------------+\n|                  id|isDeleted|                name|             pythons|season|         source_file|     processing_time|\n+--------------------+---------+--------------------+--------------------+------+--------------------+--------------------+\n|https://example.n...|    false|The Spanish Inqui...|[{https://example...|S02E02|/Volumes/hacking_...|2024-04-19 01:45:...|\n|https://example.n...|    false|The Piranha Brothers|[{https://example...|S02E01|/Volumes/hacking_...|2024-04-19 01:45:...|\n|https://example.n...|    false|     The Cheese Shop|[{https://example...|S02E02|/Volumes/hacking_...|2024-04-19 01:45:...|\n|https://example.n...|    false|   Eric the Half Bee|[{https://example...|S02E01|/Volumes/hacking_...|2024-04-19 01:45:...|\n+--------------------+---------+--------------------+--------------------+------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf = sketches.read() \n",
    "\n",
    "sdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a71645d5-1314-4c2b-849e-81a8768befb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## The Raw Delta + Spark Functions\n",
    "\n",
    "Finally, we get the same result using raw Delta and Spark functions, without Mets-Job (well, just a little help from the schema definitions), like so...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b766107e-f5be-4838-9c9a-b26c192d1607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_location = \"/Volumes/hacking_data/delta_test/checkpoints\"\n",
    "fully_qualified_table_name = \"hacking_data.delta_test.sketch\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {fully_qualified_table_name}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "(DeltaTable.createIfNotExists(spark_session)\n",
    "           .tableName(fully_qualified_table_name)\n",
    "           .addColumns(sketches_schema().hive_schema())\n",
    "           .partitionedBy((name,))\n",
    "           .execute())\n",
    "\n",
    "\n",
    "streaming_query_raw = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"json\")\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_location)\n",
    "  .schema(sketches_event_schema().hive_schema())\n",
    "  .load(\"/Volumes/hacking_data/delta_test/streaming_events\")\n",
    "  .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", checkpoint_location)\n",
    "  .trigger(availableNow=True)\n",
    "  .toTable(fully_qualified_table_name))\n",
    "\n",
    "streaming_query_raw.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Testing Spark Streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
